# RAG-Based Job Matching System (Powered by Cohere LLM)

A scalable Retrieval-Augmented Generation (RAG) system that matches job requirements against document(candidate resume in .pdf foramt) embeddings (like candidate resumes) using Cohere’s language model and ChromaDB for semantic search — all exposed via a FastAPI endpoint and Docker-ready for smooth deployment.



#  Project Structure

RAG-Based-Question-Answering-Chatbot-Powered-Cohere-LLM/
├── app/
│   ├── api.py
│   ├── extract_documents.py
│   ├── preprocess_documents.py
│   ├── create_embeddings.py
│   ├── store_embeddings.py
│   ├── query_engine.py
│   └── requirements.txt
├── Documents/
│   ├── file1.pdf
│   ├── file2.pdf...
├── test/
│   └── test_query.py
├── Dockerfile
├── .env
├── README.md
└── .gitignore






# Features

--> documents folder have multiple .pdf candidate resume/cv files  
--> i extracted text from pdf files then 
--> Preprocessing with spaCy & NLTK
--> Chunking + sentence-transformer embeddings
--> Semantic retrieval using ChromaDB
--> Query-answering powered by Cohere’s LLM
--> REST API built with FastAPI
--> Fully containerized with Docker

# Setup (Without Docker)
## 1. Clone the repo
$ git clone https://github.com/your-username/RAG-Based-Question-Answering-Chatbot-Powered-Cohere-LLM.git
$ cd RAG-Based-Question-Answering-Chatbot-Powered-Cohere-LLM

## 2. Install Python dependencies
$ cd app
$ pip install -r requirements.txt

Also run:
$ python -m nltk.downloader punkt stopwords
$ python -m spacy download en_core_web_sm

## 3. setup cohere llm api key
$ cd ..
edit .env file and replace "your-cohere-api-key" with your actual cohere llm api key (you can create it from cohere website)

## 4. Start the API
$ cd app
$ uvicorn api:app --reload

## 5. for user interface run chat_ui.py file using streamlit UI interface
$ streamlit run chat_ui.py

You can now view your Streamlit app in your browser. you may see the URL like this:-

  Local URL: http://localhost:8501
  Network URL: http://10.97.116.182:8501


## 6. test using streamlit UI 

goto http://localhost:8501/ URL and then user can enter question you can use sample_user_input.txt file text as sample question then click on submit button 
now you will see the relevent respose generated by llm based on retrieved relevent inforamtion from vetor database.




# Run with Docker

## 1. Build the image
$ cd RAG-Based-Question-Answering-Chatbot-Powered-Cohere-LLM
$ docker build -t rag-chatbot .

## 2. Run the container
$ docker run -p 8000:8000 -e COHERE_API_KEY=your-api-key rag-chatbot


## 3. Test the API
Use the Swagger UI at http://localhost:8000/docs Or use the script in test/test_query.py:

$ python test/test_query.py


# API Endpoint Summary
POST /ask Accepts a job description or query and returns the best-matched content synthesized by the LLM.

Request:

json
{
  "question": "Looking for a backend engineer experienced in AWS and Python."
}
Response:

json
{
  "question": "...",
  "answer": "...",
  "context_used": ["chunk1", "chunk2", ...]
}


# Model + Tools Used
Embedding model: all-MiniLM-L6-v2 via SentenceTransformers

Vector DB: ChromaDB (duckdb+parquet)

LLM: Cohere's command-r-plus

Frameworks: FastAPI, LangChain, spaCy, NLTK

